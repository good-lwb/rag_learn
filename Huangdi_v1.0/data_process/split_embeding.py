import numpy as np
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import re
import faiss
from tqdm import tqdm

# åŠ è½½ä¸­æ–‡Embeddingæ¨¡åž‹
print("ðŸ”§ æ­£åœ¨åŠ è½½Embeddingæ¨¡åž‹...")
model_path = "D:/Project/Blm/RAG/ChineseMedicine_QA/data/data_base/bge-small-zh-v1.5"
model = SentenceTransformer(model_path)
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆ\n")

def split_paragraphs(text):
    return [p.strip() for p in text.split('\n') if p.strip()]
def split_sentences(paragraph):
    """åˆ†å¥ï¼šä¸­æ–‡æ ‡ç‚¹åˆ†å‰²ï¼ˆç®€å•ç‰ˆï¼‰"""
    return [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', paragraph) if s.strip()]

def semantic_chunk(text, max_chunk_size=500, min_similarity=0.75):
    print("ðŸ“– æ–‡æœ¬åˆ†å¥ä¸­...")
    # 1. åˆæ­¥åˆ†æ®µå’Œåˆ†å¥
    paragraphs = split_paragraphs(text)
    all_sentences = []
    for para in paragraphs:
        all_sentences.extend(split_sentences(para))

    # 2. ç”Ÿæˆå¥å­åµŒå…¥
    print("\nðŸ”¢ ç”Ÿæˆå¥å­å‘é‡...")
    embeddings = model.encode(all_sentences)

    # 3. åŠ¨æ€èšç±»åˆ†å—ï¼ˆåŸºäºŽç›¸ä¼¼åº¦ï¼‰
    print("\nðŸ§© è¯­ä¹‰åˆ†å—åˆå¹¶ä¸­...")
    chunks = []
    current_chunk = []
    current_embedding = None

    for sentence, embedding in zip(all_sentences, embeddings):
        if not current_chunk:
            current_chunk.append(sentence)
            current_embedding = embedding
        else:# è®¡ç®—ä¸Žå½“å‰å—çš„ç›¸ä¼¼åº¦
            sim = np.dot(embedding, current_embedding)/(np.linalg.norm(embedding) * np.linalg.norm(current_embedding))

            if sim >= min_similarity and len("".join(current_chunk)) + len(sentence) <= max_chunk_size:
                current_chunk.append(sentence)
                current_embedding = (current_embedding * (len(current_chunk) - 1) + embedding)
                current_embedding /= len(current_chunk)
            else:
                chunks.append("".join(current_chunk))
                current_chunk = [sentence]
                current_embedding = embedding

    if current_chunk:
        chunks.append("".join(current_chunk))
    return chunks

print("ðŸ“‚ è¯»å–æ–‡æœ¬æ–‡ä»¶ä¸­...")
with open("../data_process/huangdi_data.txt", "r", encoding="utf-8") as f:
    texts = f.read()
    text = texts.replace("\n\n","\n")
print(f"âœ… å·²è¯»å–æ–‡æœ¬ï¼ˆé•¿åº¦ï¼š{len(text)} å­—ç¬¦ï¼‰\n")
    # print(text)

chunks = semantic_chunk(text)
# for i, chunk in enumerate(chunks):
#     print(f"ã€Chunk {i + 1}ã€‘\n{chunk}\n{'-' * 50}")

print("\nðŸ—ï¸ æž„å»ºå‘é‡æ•°æ®åº“ä¸­...")
# ç”Ÿæˆæ‰€æœ‰å—çš„å‘é‡
embeddings = model.encode(chunks, normalize_embeddings=True, show_progress_bar=True, batch_size=32)  # å½’ä¸€åŒ–å‘é‡
dimension = embeddings.shape[1]

# æž„å»ºFaissæ•°æ®åº“
print("  æ­£åœ¨æž„å»ºFAISSç´¢å¼•...")
index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
index.add(embeddings.astype('float32'))  # FAISSéœ€è¦float32æ ¼å¼

#ä¿å­˜åˆ†è¯ç»“æžœ
chunks_file = "huangdi_chunks.txt"
with open(chunks_file, "w", encoding="utf-8") as f:
    f.write("\n".join(chunks))
print(f"âœ… åˆ†å—æ–‡æœ¬å·²ä¿å­˜è‡³ {chunks_file}")

# 3. ä¿å­˜æ•°æ®åº“åˆ°ç£ç›˜
faiss.write_index(index, "huangdi_vectors.index")
print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜ï¼ˆåŒ…å« {len(chunks)} æ¡æ•°æ®ï¼‰\n")

