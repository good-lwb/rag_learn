import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from tqdm import tqdm
from modelscope import AutoModelForCausalLM, AutoTokenizer

# ========== åˆå§‹åŒ–å…¨å±€ç»„ä»¶ ==========
print("ğŸ”§ æ­£åœ¨åŠ è½½æ¨¡å‹...")

# 1. åŠ è½½æ£€ç´¢ç»„ä»¶
model_path = "data/data_base/bge-small-zh-v1.5"
index_path = "data/data_base/huangdi_vectors.index"
chunks_path = "data/data_base/huangdi_chunks.txt"

embedding_model = SentenceTransformer(model_path)
index = faiss.read_index(index_path)
with open(chunks_path, "r", encoding="utf-8") as f:
    chunks = [line.strip() for line in f if line.strip()]

# 2. åŠ è½½LLMæ¨¡å‹
model_llm_path = "../Qwen/Qwen2.5-7B-Instruct"
llm_model = AutoModelForCausalLM.from_pretrained(
    model_llm_path,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_llm_path)

print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ\n")


# ========== æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ==========
def search(query, top_k=5):
    """æ£€ç´¢ç›¸å…³æ–‡æœ¬å—ï¼ˆè‡ªåŠ¨å»é‡ï¼‰"""
    query_embedding = embedding_model.encode([query], normalize_embeddings=True)[0].astype('float32')
    distances, indices = index.search(np.array([query_embedding]), top_k)

    # å»é‡å¹¶ä¿ç•™æœ€é«˜åˆ†ç»“æœ
    unique_chunks = {}
    for idx, score in zip(indices[0], distances[0]):
        chunk = chunks[idx]
        if chunk not in unique_chunks or score > unique_chunks[chunk]:
            unique_chunks[chunk] = score

    # æŒ‰åˆ†æ•°æ’åºè¿”å›
    sorted_results = sorted(unique_chunks.items(), key=lambda x: -x[1])
    return "\n".join([chunk for chunk, _ in sorted_results])


def generate_response(query, chat_history=[]):
    """ç”Ÿæˆå¸¦ä¸Šä¸‹æ–‡è®°å¿†çš„å›ç­”"""
    # 1. æ£€ç´¢å¢å¼º
    search_results = search(query)

    # 2. æ„å»ºPromptï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰
    prompt_template = """
[ç›¸å…³èƒŒæ™¯çŸ¥è¯†]
{search_results}
************************
[å¯¹è¯å†å²]
{history}
************************
[å½“å‰é—®é¢˜]
{query}

è¯·æ ¹æ®èƒŒæ™¯çŸ¥è¯†å’Œå¯¹è¯å†å²å›ç­”é—®é¢˜ï¼š
"""

    history_str = "\n".join([f"ç”¨æˆ·ï¼š{q}\nåŠ©æ‰‹ï¼š{a}" for q, a in chat_history[-3:]])  # ä¿ç•™æœ€è¿‘3è½®
    prompt = prompt_template.format(
        search_results=search_results,
        history=history_str,
        query=query
    )

    # 3. LLMç”Ÿæˆ
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸­åŒ»åŠ©æ‰‹Qwenï¼Œè¯·ä¸“ä¸šä¸”å‹å¥½åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
        {"role": "user", "content": prompt}
    ]
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([inputs], return_tensors="pt").to(llm_model.device)

    generated_ids = llm_model.generate(
        **model_inputs,
        max_new_tokens=512,
        temperature=0.7  # æ§åˆ¶åˆ›é€ æ€§
    )
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)

    return response.strip()


# ========== å¤šè½®å¯¹è¯å¾ªç¯ ==========
def chat_loop():
    print("\nğŸŸ¢ ä¸­åŒ»é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸå¯¹è¯ï¼‰")
    history = []

    while True:
        try:
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            query = input("\nğŸ‘¤ userï¼š")
            if query.lower() in ["é€€å‡º", "exit", "quit"]:
                print("ğŸ›‘ å¯¹è¯ç»“æŸ")
                break

            # 2. ç”Ÿæˆå›ç­”
            print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
            response = generate_response(query, history)
            print(f"ğŸ” æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µï¼š\n{search(query)}\n")
            print(f"ğŸ’¡ assistantï¼š{response}")

            # 3. æ›´æ–°å†å²
            history.append((query, response))

        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­å¯¹è¯")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            continue


if __name__ == "__main__":
    chat_loop()